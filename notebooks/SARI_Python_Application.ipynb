{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARI Python Earth Science Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "In the previous session, we looked at how to read in geographic data and explore the data using Python. In this next session, we are going to analyze geographic data for an Earth science application using Python.\n",
    "\n",
    "During this application, we are going to focus on reading in spatial data in the form of a raster (image), manipulate the data, and perform calculations to extract land cover information. Then we are going to use a vector data set (point, line, or polygon) to summarize results from the raster data for both time periods and visualize the results.\n",
    "\n",
    "At the end of this exercise we will have an understanding of how to perform calculations on raster data and save our results then use that extracted information to display and interactive map that people can view.\n",
    "\n",
    "\n",
    "\n",
    "## Application\n",
    "\n",
    "We are going to focus on an urban development application using satellite imagery to extract that land cover class. This application is going to be split into two sections: (1) processing raster data and (2) processing vector data. Built-up extent information over Chiang Mai, Thailand is going to be extracted from Landsat 5 TM imagery (raster data) from both 1990 and 2010 using the methods from [*Xu* [2007]](http://info.asprs.org/publications/pers/2007journal/december/2007_dec_1381-1391.pdf). After we calculate the built-up area over we are going to use sub-district level data (vector data) over the area of study to summarize percent area of each region and visualize the change from 1990 to 2010. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named geopandas",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a0b27a37d523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfolium\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplugins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfolium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named geopandas"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "#import dependencies\n",
    "import warnings\n",
    "import folium\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from folium import plugins\n",
    "from folium.features import *\n",
    "from rasterstats import zonal_stats\n",
    "\n",
    "warnings.simplefilter('ignore') # simple filter to ignore warning messages when processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Processing raster datasets\n",
    "\n",
    "### Opening and Exploring Data\n",
    "\n",
    "Here we are going to read in the Landsat Thematic Mapper (TM) image from Apr. 3rd, 1990. This data has been preprocessed to be formatted as a seven-band image clipped to our study area shown later on in this example. As we read in the data we are going to view the dimensions of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm90 = rasterio.open('LT05_L1TP_19900403_T1_ChiangMaiComposite.TIF') # open 1990 data for reading\n",
    "print(\"Number of bands:\",cm90.count) # get the number of bands\n",
    "print(\"Image height\", cm90.height) # get the y dimension\n",
    "print(\"Image width\", cm90.width) # get the x dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the print statements this dataset has 7 bands (band1=blue, band2=green, and so on) with 2754 pixels along the x-axis and 3161 pixels along the y-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the raster data model, the x- and y-axis dimensions help provide geographic context relative to a geographic transformation. This geographic transformation information is typically defined by a point at the upper left corner of the image with the geographic width and height for each pixel accompanying that point. We are going to view the geographic transformation of the Landsat data with the projection information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = cm90.crs # save projection information to a variable\n",
    "transform = cm90.transform # save the geographic transformation to a variable\n",
    "\n",
    "# print the geographic metadata\n",
    "print('Projection information:',proj)\n",
    "print('Geographic transformation:',transform[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting geographic information can be a little cryptic if you don't know what it means...first we are going to dive deeper into the projection. There is this EPSG code; each commonly used geographic projection that is defined has an EPSG code. You can find what projection a code is by searching at this website: [www.spatialreference.org](http://spatialreference.org). From searching for the projection code, we can find that the Landsat data is in the WGS84/UTM zone 47N projection. \n",
    "\n",
    "Next, the geographic transformation is an affine matrix that has six values. These values define where the raster pixels are supposed to be on a geographic projection. Here are the affine values (Width of a pixel in projection units, left point on the x-axis, rotation on the x-axis, rotation on the y-axis,  height of a pixel in projection units, top point on the y-axis).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data format\n",
    "\n",
    "The raster data is currently a RasterIO reader variable, this means we can only read information from the dataset and not do any processing. For us to do processing we are going to read the data in as a [NumPy array](https://docs.scipy.org/doc/numpy-dev/user/quickstart.html). NumPy is a Python package that allows arithmetic to be applied on the array elementwise very efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data90 = cm90.read() # read in the raster dataset as a numpy array\n",
    "cm90.close() # close the dataset to free the memory\n",
    "print(\"Variable data type:\", type(data90)) # print the data type for the data90 variable\n",
    "print(\"Array dimensions\", data90.shape) # print the dimensions of the numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see from printing the data type, the `data90` variable is a NumPy array. Furthermore, we can see that the NumPy array has the same dimensions as the original raster data: 7 bands, 3161 pixels along the y-axis, and 2754 pixels along the x-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start doing the processing we are going to do a few preprocessing steps. First, we are going to mask nodata values. When you read in image data as a NumPy array all values, including nodata values that a GIS software mask, have a value. Landsat TM data is 8-bit data with a value range of 0-255. Therefore, any values outside of that range are nodata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data90 = np.ma.masked_where(data90>255,data90) # mask no data values\n",
    "print(\"Variable data type:\", type(data90))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we view the variable type, we can see that it changed from a NumPy array to a masked NumPy array. This means that all data values that met the criteria in the above syntax will not be processed in subsequent steps. Masking data nodata values also helps with visualization where only the data you would like will be shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point we have been processing raster data in variables without seeing the data. It is always good to view the data so you can visualize what you have been doing. Here we are going to display the near-infrared band (band 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(data90[3,:,:],cmap='gray',interpolation='nearest') # display the image\n",
    "ylabel('Y-axis pixels') # label the y-axis\n",
    "xlabel('X-axis pixels') # label the x-axis\n",
    "cb = colorbar() # display a colorbar\n",
    "cb.set_label('Digital Number') # label the colorbar\n",
    "\n",
    "# print the dynamic data range\n",
    "print(\"Minimum value: {0}\\tMaximum value: {1}\".format(data90[3,:,:].min(),data90[3,:,:].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this simple visualization, we can infer a lot of information. First, notice that the units along the x- and y-axis are in pixels. When we read in the raster as an array, we lost the geographic information associated with the image. The geographic transform and projection will be added back in later as a final step (remember we saved this data to variables earlier). Also, we can see the dynamic range of the data. The minimum value is 26 and the maximum value is 152."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "As part of the methodology outlined by *Xu* [2007], we will need to perform preprocessing step. This is a simple atmospheric correction on the Landsat image. This atmospheric correction is the Dark Object Subtract (DOS) technique given by the following equation:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\lambda_{\\mathit{cor}} = \\lambda_{\\mathit{i}} - min(\\lambda_{\\mathit{i}})\n",
    "\\end{aligned}\n",
    "\n",
    "where $\\lambda_{\\mathit{cor}}$ is the resulting corrected image for wavelength $\\mathit{i}$ and $min(\\lambda)$ is the minimum value in the image for that wavelength.\n",
    "\n",
    "More information on the DOS techniques and rationale behind the method is described by *Vincent* [1972] and *Rowan et al.* [1974].  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over each band in image array\n",
    "for i in range(data90.shape[0]):\n",
    "    do = data90[i,:,:].min() # find dark object for each band\n",
    "    data90[i,:,:] = data90[i,:,:] - do # apply DOS at each band"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have applied the DOS, we want to see the results of the workflow. We will check our output results by visualizing the data again and checking the dynamic range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(data90[3,:,:],cmap='gray',interpolation='nearest') # display a band in the corrected image\n",
    "ylabel('Y-axis pixels') # label the y-axis\n",
    "xlabel('X-axis pixels') # label the x-axis\n",
    "cb = colorbar() # display a colorbar\n",
    "cb.set_label('Digital Number') # label the colorbar\n",
    "\n",
    "# print the dynamic data range\n",
    "print(\"Minimum value: {0}\\tMaximum value: {1}\".format(data90[3,:,:].min(),data90[3,:,:].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When viewing the data, there is little noticeable change. However, the data has all been adjusted by the DO values when we print the dynamic range. Now the minimum value is 0 and the maximum value has been adjusted by the minimum value from earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last preprocessing step we are going to perform is to extract specific bands we are using in our algorithm and convert their [data type](https://en.wikibooks.org/wiki/Python_Programming/Data_Types#Built-in_Data_types). To do calculations, specifically calculations with divisions, we want the data in a float data type and we force the data to be floating type using the built-in `.astype()` function for NumPy arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # extract bands from the Landsat TM image\n",
    "grn = data90[1,:,:].astype(np.float) # get the green band (2)\n",
    "red = data90[2,:,:].astype(np.float) # get the red band (3)\n",
    "nir = data90[3,:,:].astype(np.float) # get the NIR band (4)\n",
    "swir1 = data90[4,:,:].astype(np.float) # get one of the SWIR bands (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of built-up area\n",
    "\n",
    "Now that we have performed some preprocessing on the data, we are going to perform the built-up area extraction. Per the workflow of *Xu* [2007] we first calculate three indices: (1) the normalized difference built-up index (NDBI), (2) the modified normalized difference water index (MNDWI), and (3) the soil-adjusted vegetation index.\n",
    "\n",
    "First, we are going to calculate the NDBI and visualize the results to see how calculations are done. The equation for NDBI defined as:\n",
    "\n",
    "\\begin{aligned}\n",
    "NDBI = \\dfrac{SWIR-NIR}{SWIR+NIR}\n",
    "\\end{aligned}\n",
    "\n",
    "where $SWIR$ is the short-wave infrared band (band 5 on the Landsat TM sensor) and $NIR$ is the near-infrared band (band 4 on the Landsat TM sensor). The values range for NDBI is -1 to 1 where values closer to one correspond with built-up areas. \n",
    "\n",
    "The following code will first calculate the NDBI using the above equation and then display the resulting image and the value range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndbi = (swir1-nir) / (swir1+nir) # normalized difference built-up index\n",
    "imshow(ndbi,cmap='RdBu_r',interpolation='nearest')\n",
    "colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the resulting image, we can see that the value ranges are what we expect from the calculations with areas in red correspond to built-up areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two more indices need to be calculated as part of the workflow. These are the MNDWI and SAVI. \n",
    "\n",
    "The equation for MNDWI defined as:\n",
    "\n",
    "\\begin{aligned}\n",
    "MNDWI = \\dfrac{GREEN-SWIR}{GREEN+SWIR}\n",
    "\\end{aligned}\n",
    "\n",
    "where $GREEN$ is the green band (band 2 on the Landsat TM sensor) and $SWIR$ is the short-wave infrared band (band 5 on the Landsat TM sensor). The value range for MNDWI will be -1 to 1 where values closer to one correspond to water areas.\n",
    "\n",
    "The SAVI is calculated using the following equation:\n",
    "\n",
    "\\begin{aligned}\n",
    "SAVI = \\dfrac{(NIR-RED)(1+l)}{NIR+RED+l}\n",
    "\\end{aligned}\n",
    "\n",
    "where $NIR$ is the near-infrared band (band 4 on the Landsat TM sensor), $RED$ is the red band (band 3 on the Landsat TM sensor), and $l$ is a correction factor ranging from 0 for very high vegetation densities to 1 for very low vegetation densities. The value range for SAVI is -2 to 2 but largely depends on the value for $l$ used.  Greater values correspond with vegetated areas.\n",
    "\n",
    "The following code calculates the MNDWI and SAVI using the above equations. For the SAVI calculation, a value of 0.5 is used for the coefficient $l$ to represent intermediate vegetation density for the study region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 0.5 # veg density correction factor\n",
    "mndwi = (grn-swir1) / (grn+swir1) # modified normalized difference water index\n",
    "savi = ((nir-red)*(1+l)) / (nir+red+l) # soil-adjusted vegetation index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of our index calculation we have three separate arrays for each index. For visualizations purposes, we are going to make a composite RGB image using the three indices. In the code, we assign the NDBI, SAVI, and MNDWI indices to the red, green, and blue visualization channels, respectively. In the resulting image, we should see built-up areas displayed as red, vegetated areas displayed as green, and water areas displayed as blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.dstack([ndbi,savi,mndwi]) # stack the arrays in a 3-dimensional array for visualization\n",
    "imshow(indices,interpolation='nearest') # display the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From visual interpretation of the image, we can infer where the built-up areas are. To fully extract that information, we are going to perform another calculation. This will be a simple logic-based calculation to identify built-up areas using the idea that if NDBI values are larger than both the SAVI and MNDWI then a pixel must be a built-up area. This calculation is defined as:\n",
    "\n",
    "\\begin{aligned}\n",
    "Builtup =  NDBI > SAVI \\quad  and \\quad NDBI > MNDWI\n",
    "\\end{aligned}\n",
    "   \n",
    "The following code implements the logic calculations to test for built-up areas. First, we perform two tests to find areas where NDBI is greater than the other indices. Then, we test for areas where both cases are true. The result is an image with values of 0 (False) and 1 (True), where a value of 1 means the pixel was classified as built-up area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = ndbi > savi # test that ndbi greater than savi\n",
    "test2 = ndbi > mndwi # test that ndbi greater than mndwi\n",
    "builtup90 = (test1&test2).astype(np.int16) # areas where both tests are true\n",
    "imshow(builtup90, cmap='copper',interpolation='nearest') # display resulting image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare our extracted built-up areas from the logic calculations to the RGB composite above. When comparing the two images, we see our extracted area corresponds with the red areas. This gives us confidence that our methods did what we expected by extracting areas where the NDBI values were greater than the other indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results\n",
    "\n",
    "Now that we have extracted the built-up areas from the imagery, we are going to save the results of our workflow in a GeoTiff dataset. This output dataset will then be able to be used in other GIS software for visualization or other processing.\n",
    "\n",
    "To do this we must add in our geographic information again (remember NumPy arrays are in arbitrary space!). The RasterIO package allows us to write the image to a GeoTiff file and add in metadata with the geographic information using simple commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open geotiff file for writing and provide geographic information\n",
    "builtup_out = rasterio.open('ChiangMai_builtup_1990.TIF','w', \n",
    "                           driver='GTiff', height=builtup90.shape[0], width=builtup90.shape[1],\n",
    "                           count=1, dtype=np.int16, crs=proj, transform=transform, nodata=0)\n",
    "\n",
    "builtup_out.write(builtup90,1) # write the data to the first band\n",
    "builtup_out.close() # flush"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the 2010 Data\n",
    "\n",
    "Now that we have gone through and extracted the built-up area from the 1990 imagery, we will need to perform the methodology on the 2010 imagery. Since we now know how this is done, we are going to execute all of the code needed for the algorithm at once for the 2010 case. Again, this code reads in the raster data, performs the preprocessing steps (mask nodata and DOS), calculates the needed indices, extracts built-up area, and finally saves the results to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm10 = rasterio.open('LT05_L1TP_20100325_T1_ChiangMaiComposite.TIF') # open 2010 data for reading\n",
    "\n",
    "data10 = cm10.read() # read in the raster dataset as a numpy array\n",
    "cm10.close() # close the dataset to free the memory\n",
    "\n",
    "data10 = np.ma.masked_where(data90>255,data10) # mask nodata values\n",
    "\n",
    "# loop over each band in image array for DOS\n",
    "for i in range(data10.shape[0]):\n",
    "    do = data10[i,:,:].min() # find dark object for each band\n",
    "    data10[i,:,:] = data10[i,:,:] - do # apply DOS at each band\n",
    "    \n",
    "# extract bands from the Landsat TM image\n",
    "grn = data10[1,:,:].astype(np.float) # get the green band (2)\n",
    "red = data10[2,:,:].astype(np.float) # get the red band (3)\n",
    "nir = data10[3,:,:].astype(np.float) # get the NIR band (4)\n",
    "swir1 = data10[4,:,:].astype(np.float) # get one of the SWIR bands (5)\n",
    "\n",
    "# calculate indices\n",
    "l = 0.5 # veg density correction factor\n",
    "ndbi = (swir1-nir) / (swir1+nir) # normalized difference built-up index\n",
    "mndwi = (grn-swir1) / (grn+swir1) # modified normalized difference water index\n",
    "savi = ((nir-red)*(1+l)) / (nir+red+l) # soil-adjusted vegetation index\n",
    "\n",
    "# extract built-up areas\n",
    "test1 = ndbi > savi # test that ndbi greater than savi\n",
    "test2 = ndbi > mndwi # test that ndbi greater than mndwi\n",
    "builtup10 = (test1&test2).astype(np.int16) # areas where both tests are true\n",
    "\n",
    "# save out results\n",
    "# open geotiff file for writing and provide geographic information\n",
    "builtup_out = rasterio.open('ChiangMai_builtup_2010.TIF','w', \n",
    "                           driver='GTiff', height=builtup10.shape[0], width=builtup10.shape[1],\n",
    "                           count=1, dtype=np.int16, crs=proj, transform=transform, nodata=0)\n",
    "\n",
    "builtup_out.write(builtup10,1) # write the data to the first band\n",
    "builtup_out.close() # flush"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now have two new GeoTiff files in our folders named \"ChiangMai_builtup_YYYY.TIF\", where YYYY is the year that the data is associated with. These GeoTiff files can now be used in GIS software such as QGIS or ArcGIS for further analysis and visualizations.\n",
    "\n",
    "The raster data is now all processed. We are now going to use vector data to analyze the changes in built-up area from 1990 to 2010."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processing vector datasets\n",
    "\n",
    "### Opening and Exploring Data\n",
    "\n",
    "As with most processing, we start with reading in the dataset. We will use the GeoPandas package again to do our vector processing. The following code opens up sub-district data over the Chiang Mai region (which overlaps geographically with the Landsat imagery used in the previous section). When we read in the dataset, we are going to print the first ten rows of the dataset so we can see what the data comprises of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chiangmai = gpd.read_file('chiangmai_subdistrict.shp') # open the vector dataset\n",
    "chiangmai.head(10) # print the first 10 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see from the printed results, this data has 22 columns of data including information on the district level name, type of district, and geographic shape/area for each feature in the dataset. Now we want to see how many features (rows) this dataset has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of features:\",chiangmai.index.size) # print total number or features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that there are a total of 105 features in the vector dataset. It is always good to see what kind of data you have and how the data is formatted. Now, let's visualize the data so we can see the geographic area that we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chiangmai.plot() # plot the vector data\n",
    "plt.xlabel(\"Longitude\") # label x axis\n",
    "plt.ylabel(\"Latitude\") # label y axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can see that the vector dataset has the same shape as the raster dataset, so they should align...However, notice the units on the x- and y-axis. These units are in decimal degrees, meaning the projection is a Geographic Coordinate System."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Geographic Projections\n",
    "\n",
    "Remember that the raster dataset projection was in a UTM Zone 47N projection with units in meters. The raster projection does not align with the vector projection. If you are familiar with GIS processing software, such as ArcGIS or QGIS, you know that the software handles \"projecting on the fly\" for processing, this does not happen when using Python for geoprocessing. You will have to be careful to make sure all data is in the same geographic projection before performing any calculations.\n",
    "\n",
    "We now have to make sure that the geographic projections align between the raster and vector dataset so we can do calculations. Python uses the EPSG codes for projection information (as seen from the raster example above) so first, we are going to print the EPSG code to make sure we get the correct projection information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vector projection:',chiangmai.crs) # check projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were to search for this EPSG code, you can see that the projection is the WGS84 coordinate system. This is a very common coordinate system for global datasets. Again, this projection does not match the raster dataset's project. We are going to need to do some reprojecting, and the following code will do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define projection EPSG codes...\n",
    "utm47n_proj = {'init': 'epsg:32647'} # ...for UTM Zone 47N\n",
    "wgs84_proj = {'init' :'epsg:4326'} # ...for WGS84\n",
    "\n",
    "chiangmai = chiangmai.to_crs(utm47n_proj) # perform a reprojection\n",
    "print(\"New projection:\",chiangmai.crs) # check projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from printing the new projection information that the EPSG code matches the raster dataset's. We are going to display the vector data to see if any changes were made to the dataset while reprojecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chiangmai.plot() # plot the vector data\n",
    "plt.xlabel(\"Easting [m]\")\n",
    "plt.ylabel(\"Northing [m]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, the reprojection did not change the vector features, however, notice the change in projection units. The original units were in decimal degrees (latitude and longitude) and now the units are in meters. Now we are confident that the projections between the raster and vector data match, we can perform the geoprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zonal Statistics and Field Calculations\n",
    "\n",
    "We are going to use zonal statistics to summarize the raster information on a feature basis within the vector dataset. In Python there is a powerful package to perform zonal statistical calculations with raster and vector dataset; this is the [rasterstats package](http://pythonhosted.org/rasterstats/). We will use the `zonal_stats()` function, the inputs are the vector dataset, the raster dataset in NumPy array format, and the affine transformation for the raster data. The following code performs the zonal statistics geoprocessing for the 1990 built-up raster dataset and shows the results for the first ten features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stats90 = zonal_stats(chiangmai, builtup90,affine=transform) # do the zonal statistics\n",
    "stats90[:10] # display the results from the first ten features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default results from the process are the number of pixels in a feature (count), the maximum pixel value in a feature (max), the average pixel value in a feature (mean), and minimum pixel value in a feature (min). The output statistics for each feature can be modified, more information to change the output statistics can be found at the rasterstats documentation. \n",
    "\n",
    "If you notice, the resulting zonal statistics are in a different format from the actual vector data. The output is a [list](https://www.tutorialspoint.com/python/python_lists.htm) of [dictionaries](https://www.tutorialspoint.com/python/python_dictionary.htm) but the vector data is in a DataFrame format. We are going to need to convert the zonal statistics data to a DataFrame and perform a table join to add the resulting zonal statistics data to the vector data. The following code performs the data conversion and join while saving the output to a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_vector = chiangmai.join(pd.DataFrame(stats90)) # join tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a `join_vector` variable that holds the original geographic and attribute data from earlier and now has the computed zonal statistics data. We are now going to use the data fields (columns) from `join_vector` to calculate new fields in the original vector dataset. We do this by calling the field name in each dataset, if we assign a field name it will be created automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chiangmai['bu_pct90'] = join_vector['mean'] # add new field for percent area that is built-up\n",
    "chiangmai['bu_area90'] = (chiangmai['bu_pct90'] * chiangmai.area) / 10e6  # convert built-up percent to area in sq km\n",
    "print(chiangmai['bu_area90'][:10]) # print the first ten features built-up area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be wondering, \"Why did we not just join the zonal statistics outputs to the `chiangmai` variable?\" We did not do this because we need to calculate the same statistics/fields using the 2010 built-up raster dataset for comparison. Vector datasets only allow one unique field name, such as 'count', in a table. To get around this, we saved the joined tables to a new variable and passed that same information to our original variable as above.\n",
    "\n",
    "Before we calculate the 2010 built-up zonal statistics, we have to clear the `join_vector` variable. We do this using the below code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_vector = None # assign the variable to be nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to calculate the statistics and fields using the 2010 built-up raster. We do this with the same workflow above: calculate the zonal statistics, join the tables, calculate new fields in the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats10 = zonal_stats(chiangmai, builtup10,affine=transform) # do the zonal statistics\n",
    "join_vector = chiangmai.join(pd.DataFrame(stats10)) # join the tables\n",
    "chiangmai['bu_pct10'] = join_vector['mean']\n",
    "chiangmai['bu_area10'] = (chiangmai['bu_pct10'] * chiangmai.area) / 10e6                          \n",
    "\n",
    "join_vector = None # assign the variable to be nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have built-up area and percent area data for each feature from the 1990 and 2010 time periods in our original dataset. For our application, we want to show the difference from 1990 to 2010. We do this, again, by using a simple subtraction between the two time periods resulting in new difference fields show using the syntax below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chiangmai['bu_areadiff'] = chiangmai['bu_area10'] - chiangmai['bu_area90'] # difference in built-up area\n",
    "chiangmai['bu_pctdiff'] = chiangmai['bu_pct10'] - chiangmai['bu_pct90'] # difference in built-up percent area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally have all of the calculated data for our application. The results from the above code have data showing increases or decreases in built-up area/percent area from 1990 to 2010."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Vectors and Making Web Maps\n",
    "\n",
    "After we have all performed all of the processing, we want to save the file. We do this using the below syntax. Before we save the file, we are going to reproject the data back to its original projection, WGS84. We use the same syntax as the previous reprojection but give it the WGS84 EPSG code. Then we finally save the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproject the joined vector data to a WGS84 projection\n",
    "out_vector = chiangmai.to_crs(wgs84_proj)\n",
    "\n",
    "# save the data to a geojson\n",
    "out_vector.to_file('chiangmai_builtup.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a new shapefile in our folders called \"chiangmai_builtup.shp\" that we can use with other GIS software for further analysis or visualization.\n",
    "\n",
    "Our last step for this application is to convert the processed data into a web map so we can show our users the application. Python provides an easy to use interactive map creation package, [Folium](https://folium.readthedocs.io/en/latest/). This package allows you to create interactive maps in your Python environment and save the maps as a webpage for displaying.\n",
    "\n",
    "We are going to create the map using the below syntax. For this web map, the difference in percent area from 1990 to 2010 is going to be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "map_ = folium.Map(location=[18.75, 99], zoom_start=9,tiles='cartodbpositron') # initialize basemap and start position\n",
    "\n",
    "# create chorpleth map and define visualization parameters\n",
    "map_.choropleth(geo_str = out_vector.to_json(), data = out_vector,\n",
    "                columns = ['OBJECTID', 'bu_pctdiff'], key_on = 'feature.properties.{}'.format(\"OBJECTID\"),\n",
    "                fill_color = 'RdGy', fill_opacity = 0.8, line_opacity = 0.9,\n",
    "                legend_name='Pecent Built-up Difference',highlight=True)\n",
    "\n",
    "map_.add_children(folium.map.LayerControl()) # add layer control functions\n",
    "\n",
    "map_ # add map inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we now can zoom, pan, click, and view the results on a nice interactive map. However, we want a map that we can display as its own webpage, we do this by saving the map to a HTML file. We do this with the following syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_.save('chiangmai_builtup.html') # save the map to html page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is now a 'chiangmai_builtup.html' file in our folder. If we open this file, we are shown a webpage with the interactive map. This is helpful as you can use Python to create you web maps and use the webpages on your own webserver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In conclusion, we performed an Earth science application using Python. This application focused on using remotely sensed datasets (Landsat TM) to extract areas that are built-up from two time periods, 1990 and 2010. We then used a vector dataset to summarize the built-up area from the two time periods over sub-district areas. Lastly, we created an interactive web map of the vector dataset showing the increase or decrease in built-up area from the two time periods.\n",
    "\n",
    "This application highlighted the utility of Python to perform geoprocessing workflows and create information that end-users can then access for policy formation. What kind of science applications will you perform with Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Rowan, L. C. et al. (1974), Discrimination of rock types and detection of hydrothermally altered areas in south-central Nevada by the use of computer-enhanced ERTS images, U.S. Geological  Survey  Professional Paper 883. [Link](https://pubs.usgs.gov/pp/0883/report.pdf)\n",
    "\n",
    "Vincent, R. K. (1973), Spectral ratio imaging methods for geological remote sensing from aircraft and satellites, In *Proceedings of the American Society of Photogrammetry, Management and Utilization of Remote Sensing Data Conference*, Sioux Falls,  SD, pp. 377-397. [Link](https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19730001633.pdf)\n",
    "\n",
    "Xu, H. (2007), Extraction of Urban Built-up Land Features from Landsat Imagery Using a Thematic oriented Index Combination Technique, *Photogramm. Eng. Rem. S., 73*(12), 1381â€“1391. [Link](http://info.asprs.org/publications/pers/2007journal/december/2007_dec_1381-1391.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
